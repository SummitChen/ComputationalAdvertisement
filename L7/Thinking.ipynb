{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1. 在CTR点击率预估中，使用GBDT+LR的原理是什么？*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT + LR 的基本原理是将CTR任务的训练分成两个阶段：1. 特征抽取阶段 2. 基于抽取的特征进行预测。GBDT采用梯度迭代（Gradient Boosting）方法将多颗CART树叠加起来预测结果，且每一颗子树都是对结果的部分预测。因此可将每颗子树的输出拼接起来看做是对原始数据一种特征编码，然后将编码后的特征通过线性回归来进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2. Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide&Deep 是将线性模型和深度模型拼接起来进行训练和预测的一种模型。所谓的记忆能力(memorization)即线性模型可学习到数据的显性特征，往往根据显性特征能够对预测结果记性解释，而泛化能力(generalization)即深度学习模型可学习到的特征与特征之间的内在联系，即隐形特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3. 在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CTR预估中，FM与DNN的结合方式主要有两种，一种是\"串行\"方式，代表模型有FNN (Factorisation-machine supported Neural Networks),在此模型中，稀疏特征先通过Dense Embedding进行编码，而Dense Embedding层的参数是由FM模型初始化的，计算得到的结果再进入DNN进行高阶特征的抽取和计算；另外一种是“并行”方式，代表模型有DeepFM,在此模型中，稀疏特征经过Dense Embedding层，一路进入FM层，用来计算二阶特征；另外一路进入DNN中计算高阶特征，其中FM层的一阶特征是由稀疏特征经过线性计算得到。最后FM层的输出结果与DNN的输出结果拼接起来一起进入DeepFM的输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *4. Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline的算法原理所基于的假设是某一用户u对物品i的评价$\\hat{r}_{ui}$是由物品的所有物品收到的平均评价和物品i获得评价的偏差以及用户打分偏差之和计算得到，即：\n",
    "$$b_{ui} = \\mu + b_u + b_i $$\n",
    "其中$b_u$和$b_i$可通过交替最小二乘ALS优化以下函数得到：\n",
    "$$\\min_{b_*}\\sum_{(u,i)\\in K}(r_{ui} - \\mu - b_u - b_i)^2 + \\lambda_1(\\sum_{u}b_u^2 + \\sum_{i}b_i^2)$$\n",
    "在此基础上KNNBaseline在计算$\\hat{r}_{ui}$时考虑到了用户u的k个邻居用户对物品i的评价\n",
    "$$\\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{v\\in{N_i}^k(u)}sim(u, v)\\cdot(r_{vi} - b_{vi})}{\\sum_{v\\in{N_i}^k(u)}sim(u,v)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *5.GBDT和随机森林都是基于树的算法，它们有什么区别？* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT和随机森林都是基于树模型，且都属于集成学习方法的一种。它们的不同之处在于：\n",
    "- GBDT采用的是Boosting方法，即通过将弱学习器提升为强学习器的集成方法来提高预测精度。\n",
    "- 而随机森林采用的是Bagging方法，即通过自助采样的方法生成众多并行式的分类器，通过“少数服从多数”的原则来确定最终的结果。\n",
    "\n",
    "直观的解释可以认为GBDT是“串行”的方法，“下游”学习器要基于“上游”学习器的结果进一步学习，而随机森林则采用的是“并行”的方法，每个子学习器彼此独立，最近结果要将所有学习器的结果综合得出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *6. 基于邻域的协同过滤都有哪些算法，请简述原理？*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于领域的协同过滤方法有： \n",
    "1. 基于用户的协同过滤（UserCF）- 通过当前用户u与其相似用户v的相似度，以及相似用户对兴趣物品i的兴趣程度，来推测当前用户u对物品i的兴趣程度。按照这种方法来计算与用户u相似的k个用户的所有兴趣物品，然后依据用户u对它们兴趣程度大小，来进行推荐。\n",
    "2. 基于物品的协同过滤（ItemCF）- 此方法通过用户u对物品i的k邻居物品的兴趣度来计算用户u对物品i的兴趣度；然后根据用户u对候选物品列表中的候选物品的兴趣程度大小来给用户u进行推荐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37664bittfconda946bd5c5de684c5d81ef2ce52df4450d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
