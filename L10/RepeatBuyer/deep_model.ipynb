{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "user_log = pd.read_csv('./data_format1_small/sample_user_log.csv', dtype={'time_stamp': 'str'})\n",
    "user_info = pd.read_csv('./data_format1_small/sample_user_info.csv')\n",
    "train_data1 = pd.read_csv('./data_format1_small/train.csv')\n",
    "submission = pd.read_csv('./data_format1_small/test.csv')\n",
    "\n",
    "# load Full data\n",
    "# user_log = pd.read_csv('./data_format1/user_log_format1.csv', dtype={'time_stamp': 'str'})\n",
    "# user_info = pd.read_csv('./data_format1/user_info_format1.csv')\n",
    "# train_data1 = pd.read_csv('./data_format1/train_format1.csv')\n",
    "# submission = pd.read_csv('./data_format1/test_format1.csv')\n",
    "\n",
    "train_data = pd.read_csv('./data_format2/train_format2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat train and test data for preprocessing\n",
    "train_data1['origin'] = 'train'\n",
    "submission['origin'] = 'test'\n",
    "matrix = pd.concat([train_data1, submission], ignore_index = True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regulize data type in user_log\n",
    "user_log.rename(columns = {'seller_id' : 'merchant_id'}, inplace=True)\n",
    "\n",
    "user_log['user_id'] = user_log['user_id'].astype('int32')\n",
    "user_log['merchant_id'] = user_log['merchant_id'].astype('int32')\n",
    "user_log['item_id'] = user_log['item_id'].astype('int32')\n",
    "user_log['cat_id'] = user_log['cat_id'].astype('int32')\n",
    "user_log['brand_id'].fillna(0, inplace=True)\n",
    "user_log['brand_id'] = user_log['brand_id'].astype('int32')\n",
    "user_log['time_stamp'] = pd.to_datetime(user_log['time_stamp'], format='%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe_merchant_id = LabelEncoder()\n",
    "lbe_merchant_id.fit(np.r_[0, user_log['merchant_id'].values])\n",
    "user_log['merchant_id'] = lbe_merchant_id.transform(user_log['merchant_id'])\n",
    "matrix['merchant_id'] = lbe_merchant_id.transform(matrix['merchant_id'])\n",
    "\n",
    "lbe_user_id = LabelEncoder()\n",
    "user_log['user_id'] = lbe_user_id.fit_transform(user_log['user_id'])\n",
    "user_info['user_id'] = lbe_user_id.transform(user_info['user_id'])\n",
    "matrix['user_id'] = lbe_user_id.transform(matrix['user_id'])\n",
    "\n",
    "lbe_item_id = LabelEncoder()\n",
    "user_log['item_id'] = lbe_item_id.fit_transform(user_log['item_id'])\n",
    "\n",
    "lbe_cat_id = LabelEncoder()\n",
    "user_log['cat_id'] = lbe_cat_id.fit_transform(user_log['cat_id'])\n",
    "\n",
    "lbe_brand_id = LabelEncoder()\n",
    "user_log['brand_id'] = lbe_brand_id.fit_transform(user_log['brand_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = matrix.merge(user_info, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  merchant_id label origin  prob  age_range  gender\n",
      "0        16497         1203   0.0  train   NaN          0       1\n",
      "1         1950          946   0.0  train   NaN          2       0\n",
      "2        10829         2278   0.0  train   NaN          3       0\n",
      "3         7974          951   0.0  train   NaN          0       1\n",
      "4        14604         1892   0.0  train   NaN          7       0\n",
      "...        ...          ...   ...    ...   ...        ...     ...\n",
      "23888     2157         1748   nan   test   0.0          0       0\n",
      "23889     2673          798   nan   test   0.0          3       0\n",
      "23890    11847          639   nan   test   0.0          2       1\n",
      "23891    11847         3953   nan   test   0.0          2       1\n",
      "23892    19079         2954   nan   test   0.0          4       0\n",
      "\n",
      "[23893 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown\n",
    "matrix['age_range'].fillna(0, inplace=True)\n",
    "# 0:female, 1:male, 2:unknown\n",
    "matrix['gender'].fillna(2, inplace=True)\n",
    "matrix['age_range'] = matrix['age_range'].astype('int8')\n",
    "matrix['gender'] = matrix['gender'].astype('int8')\n",
    "matrix['label'] = matrix['label'].astype('str')\n",
    "matrix['user_id'] = matrix['user_id'].astype('int32')\n",
    "matrix['merchant_id'] = matrix['merchant_id'].astype('int32')\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# garbage collection\n",
    "del user_info, train_data1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = user_log.groupby(['user_id'])\n",
    "temp = groups.size().reset_index().rename(columns={0:'u1'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "\n",
    "kmeans = KMeans(n_clusters = 20)\n",
    "\n",
    "temp = groups['item_id', 'cat_id', 'merchant_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'u2', 'cat_id':'u3', 'merchant_id': 'u4', 'brand_id':'u5'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "\n",
    "# time interval\n",
    "temp = groups['time_stamp'].agg([('F_time', 'min'), ('L_time', 'max')]).reset_index()\n",
    "temp['u6'] = (temp['L_time'] - temp['F_time']).dt.seconds/3600\n",
    "matrix = matrix.merge(temp[['user_id', 'u6']], on='user_id', how='left')\n",
    "\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'u7', 1:'u8', 2:'u9', 3:'u10'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "\n",
    "fill_non_cols = ['u6', 'u7', 'u8', 'u9', 'u10']\n",
    "\n",
    "matrix[fill_non_cols] = matrix[fill_non_cols].fillna(0.0)\n",
    "\n",
    "matrix['u_c'] = kmeans.fit_predict(matrix[['u1','u2','u3','u4','u5','u6','u7','u8','u9','u10']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 商家特征处理\n",
    "groups = user_log.groupby(['merchant_id'])\n",
    "# 商家被交互行为数量 m1\n",
    "temp = groups.size().reset_index().rename(columns={0:'m1'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的user_id, item_id, cat_id, brand_id 唯一值\n",
    "temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "\n",
    "# 统计商家被交互的action_type 唯一值\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'m6', 1:'m7', 2:'m8', 3:'m9'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 按照merchant_id 统计随机负采样的个数\n",
    "temp = train_data[train_data['label']==-1].groupby(['merchant_id']).size().reset_index().rename(columns={0:'m10'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "\n",
    "fill_non_cols = ['m6', 'm7', 'm8', 'm9', 'm10']\n",
    "\n",
    "matrix[fill_non_cols] = matrix[fill_non_cols].fillna(0.0)\n",
    "\n",
    "matrix['m_c'] = kmeans.fit_predict(matrix[['m1','m2','m3','m4','m5','m6','m7','m8','m9','m10']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = user_log.groupby(['user_id', 'merchant_id'])\n",
    "temp = groups.size().reset_index().rename(columns={0:'um1'}) #统计行为个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'um5', 1:'um6', 2:'um7', 3:'um8'})#统计不同action_type唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['time_stamp'].agg([('first', 'min'), ('last', 'max')]).reset_index()\n",
    "temp['um9'] = (temp['last'] - temp['first']).dt.seconds/3600\n",
    "temp.drop(['first', 'last'], axis=1, inplace=True)\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left') #统计时间间隔\n",
    "\n",
    "fill_non_cols = ['um5', 'um6', 'um7', 'um8', 'um9']\n",
    "\n",
    "matrix[fill_non_cols] = matrix[fill_non_cols].fillna(0.0)\n",
    "\n",
    "matrix['um_c'] = kmeans.fit_predict(matrix[['um1','um2','um3','um4','um5','um6','um7','um8','um9']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.get_dummies(matrix['age_range'], prefix='age')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "temp = pd.get_dummies(matrix['gender'], prefix = 'g')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "matrix.drop(['age_range', 'gender'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe_action_type = {0:1, 1:2, 2:3, 3:4}\n",
    "user_log['action_type'] = user_log['action_type'].map(lbe_action_type)\n",
    "temp = pd.DataFrame(user_log.groupby('user_id')['merchant_id', 'action_type'].agg(lambda x: list(x)))\n",
    "temp.columns = ['hist_merchant_id', 'hist_action_type']\n",
    "matrix = matrix.merge(temp, on=['user_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 500\n",
    "for feature in ['hist_merchant_id', 'hist_action_type']:\n",
    "    matrix[feature] = matrix[feature].map(lambda x: np.array(x + [0]*(M - len(x)))[:M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis = 1)\n",
    "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis = 1)\n",
    "train_X, train_y = train_data.drop(['label'], axis = 1), train_data['label']\n",
    "del temp, matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "DeepCTR version 0.7.4 detected. Your version is 0.7.3.\n",
      "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.7.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from deepctr.inputs import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models import DIN, DIEN, DSIN\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X['action_type'] = np.random.randint(low=1, high=4, size=len(train_X.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14488\n",
      "1856\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "for column in train_X.columns:\n",
    "    if column != 'hist_merchant_id' and column != 'hist_action_type':\n",
    "#         print(column)\n",
    "        num = train_X[column].nunique()\n",
    "        if num > 10000:\n",
    "            dim = 10\n",
    "        else:\n",
    "            if num > 1000:\n",
    "                dim = 8\n",
    "            else:\n",
    "                dim = 4\n",
    "#         print(num)\n",
    "        if column == 'user_id':\n",
    "            feature_columns += [SparseFeat(column, num+1, embedding_dim=dim)]\n",
    "            print(num)\n",
    "        elif column == 'merchant_id':\n",
    "            feature_columns += [SparseFeat(column, num+1, embedding_dim=dim)]\n",
    "            print(num)\n",
    "        elif column == 'action_type':\n",
    "            feature_columns += [SparseFeat(column, num+1, embedding_dim=dim)]\n",
    "            print(num)\n",
    "        else:\n",
    "            feature_columns += [DenseFeat(column, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns += [VarLenSparseFeat(SparseFeat('hist_merchant_id', train_X['merchant_id'].nunique() + 1, embedding_dim=8), maxlen=M),\n",
    "                   VarLenSparseFeat(SparseFeat('hist_action_type', train_X['action_type'].nunique() + 1, embedding_dim=4), maxlen=M)]\n",
    "\n",
    "hist_features = ['merchant_id', 'action_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'attention_sequence_pooling_layer/local_activation_unit/kernel:0' shape=(40, 1) dtype=float32>\n",
      "  <tf.Variable 'attention_sequence_pooling_layer/local_activation_unit/bias:0' shape=(1,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "model = DIN(feature_columns, hist_features)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['binary_crossentropy'])\n",
    "feature_names = list(train_X.columns)\n",
    "train_model_input = {name:train_X[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 17837/17837 [00:00<00:00, 2980985.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 17837/17837 [00:00<00:00, 3577382.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14269 samples, validate on 3568 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for fea in ['hist_merchant_id', 'hist_action_type']:\n",
    "    l = []\n",
    "    for i in tqdm(train_model_input[fea]):\n",
    "        l.append(i)\n",
    "    train_model_input[fea]=np.array(l)\n",
    "history = model.fit(train_model_input, train_y, verbose=True, epochs=10, validation_split=0.2, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37664bittfconda946bd5c5de684c5d81ef2ce52df4450d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
