{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 什么是AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoder是Encoder-Decoder架构的特殊形式，它有如下几个特点：\n",
    "1. Encoder 和 Decoder 的结构是相同的；\n",
    "2. 输入数据与Label数据是完全相同的；\n",
    "3. 通过最小化输入数据与输出数据的差来训练；\n",
    "4. 最后将训练出来的Encoder-Decoder模型的Decoder部分去掉，剩下的Encoder部分就是得到的自编码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Greedy Search与Beam Search的区别是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy Search 和 Beam Search 的主要区别在于每一步最佳值的带宽。\n",
    "\n",
    "假设有一个翻译任务，要将“在九月份简访问了非洲”翻译为英文。我们采用Seq2Seq + Beam Search(B=3)来进行搜索。\n",
    "\n",
    "- 第一步： 通过模型推理我们从10000个候选词中得到了概率最大的前三个词（因为是B=3），分别为in, jane, september。\n",
    "- 第二步： 基于以上三个词，分别推理下一个与当前词汇形成的词汇组合。形成的可能词汇组合有 B*10000 = 30000。我们同样选取前三个，假设得到 in september, jane is, jane visits.\n",
    "- 第三步： 按照第二步的规则，我们继续搜索，假设得到了 in september jane, jane is visiting, jane visits Africa。\n",
    "\n",
    "继续以上步骤，直到遇到居委符号。假设我们得到了 In Septmeber Jane visits Africa. Jane is visiting Afria in September. Jane visits Africa in September. 综合评定显然第三个句子 Jane visits Africa in September.\n",
    "\n",
    "而Greedy Search 相当于 B=1 的 Beam Search 是 一种特殊的Beam Search。 如果我们用Greedy Search 进行搜索的话，第一步得到的是 in 这个词，继续往下搜索就只能得到 In September Jane visits Africa. 这个结果，虽然正确但是不符合英语的语言习惯。\n",
    "\n",
    "Greedy Search 虽然快速，但是容易陷入到局部最优解中；而Beam Search 通过加大每一步最佳值的带宽的方式来尽量搜索全局最优值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 我们为什么要用Attention机制，目的是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention机制的核心原理是学习一个权重，然后在预测阶段得到有加权的推理结果。\n",
    "\n",
    "在机器翻译领域加入Attention机制：\n",
    "\n",
    "- 可以让模型集中在所有对于当前目标单词重要的输入信息上，预测结果大大提升；\n",
    "- 通过观察Attention权重矩阵的变化，可以更好得了解哪部分翻译对应哪部分的原文文字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 什么是Self-Attention和 Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention: Self-Attention, 分别在source端和target端计算Attention, 分别捕捉source端和target端自身的词与词之间的依赖关系；然后再把source端得到的self attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
    "\n",
    "Multi-head Attention: Multi-head Attention是在Self-Attention的基础上进行改进，所基于的假设是词与词之间的依赖关系不只是在一个空间维度上存在，而可能在不同的空间维度有不同的依赖关系，所以将模型分为多个头，让模型去关注不同方面的信息，然后再将从不同的头中得到的信息合并起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37664bittfconda946bd5c5de684c5d81ef2ce52df4450d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
