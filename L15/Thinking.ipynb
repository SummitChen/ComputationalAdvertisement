{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. 机器学习中的监督学习、非监督学习、强化学习有何区别**？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三种学习方式的主要区别在于是否有Label以及Label的构成方式的不同：\n",
    "1. 监督学习，是有Label的学习；且Label是由人工产生的。在学习的过程中，通过优化方法优化模型参数来缩小预测结果与人工Label的差距来实现学习。如Figure1中所示，学习的任务是对图片进行分类，判断是否是鸭子。在左图数据中标注出来的Duck和Not Duck就是人工给的Label。\n",
    "2. 无监督学习, 是无Label的学习；在学习的过程中，通过算法自动发现数据中内在的规律来实现学习。如Figure1中所示，右图中没有给图片任何Label信息，模型自动将图片进行分类。\n",
    "3. 强化学习，是一种特殊的有Label的学习方式；Label不是由人工产生的，而是由Agent与环境的交互来产生的，又可以成为Reward。强化学习的学习方式是通过优化Action的策略以获得最大Reward来进行学习的。如Figure2中所示，老鼠(Agent)通过对迷宫（环境）的观察获取当前的状态，然后根据当前的状态判断做出Action。当自己的Action从环境中获得了Reward（奶酪）反馈之后，就会根据反馈去优化自己的行为策略，使得在这个策略的指导下，在当前的环境（游戏规则）中，获得最大的Reward。\n",
    "\n",
    "![Supervised](./images/S_Un_Supervised.png)\n",
    "\n",
    "***Figure 1. Supervised and UnSupervised Learning***\n",
    "\n",
    "![RL](./images/RL.png)\n",
    "\n",
    "***Figure 2. Reinforcement Learning ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. 什么是策略网络，价值网络，有何区别**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 策略网络：就是根据给定的输入状态通过计算给出一个确定输出的网络。比如（动作1，状态2），（动作2，状态4）。\n",
    "- 价值网络：就是根据给定的输入状态通过计算评估当前状态的价值。价值大小可通过有多大概率获得多少奖励反馈来评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCTS（蒙特卡洛搜索）的基本思想是通过多次模拟博弈过程，根据每次模拟的最终输赢结果，来搜索出一个最优的策略树。每个节点表示了一个局面，它保存了当前的状态信息，“价值”信息用A/B表示，代表了被访问B次，获胜了A次的概率。\n",
    "\n",
    "- Selection: 从根节点往下走，每次都选择一个“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续走法的节点，比如Figure3中的3/3节点。其中“节点的价值”通过UCB（Upper Confidence Bound）算法来评估，UCB算法的价值评估函数平衡了搜索-利用问题。\n",
    "- Expansion: 给选定的节点（3/3）加上一个0/0子节点，即是对当前的“未扩展的子节点”进行扩展。\n",
    "- Simulation：使用快速走子策略（Rollout Policy）走到底，得到一个胜负结果。\n",
    "- Backpropagation: 把模拟的结果0/1 或者 1/1 （在Figure3中的例子是0/1）加到它所有的父节点上。\n",
    "\n",
    "![MCTS](./images/MCTS.png)\n",
    "\n",
    "***Figure3.Monte Carlo Tree Search***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对问题进行强化学习建模的时候，首先要考虑的是强化学习的基本要素即 State, Action 和 Reward。在抖音信息流推荐中，\n",
    "- State：可以将当前用户与视频交互的时间序列行为，用户的人口学信息，设备信息上下文信息等，作为用户当前的一个状态。\n",
    "- Action: 可以将系统将某视频推荐给当前用户的行为作为一个action。\n",
    "- Reward: 用户对被推荐的视频的操作反馈，比如点击，观看，收藏，点赞，评论等作为Reward反馈给系统。\n",
    "\n",
    "在此基础上, 考虑强化学习的总体架构；比如是采用Model-based 还是 model-free 架构(参照Figure4)；以及是否要引入模拟训练机制，以辅助和加快RL算法的训练，比如AlphaGo Zero中引入的MCTS（参照Figure5）。\n",
    "\n",
    "![model](./images/model.png)\n",
    "\n",
    "***Figure4.Model-based and model-free***\n",
    "\n",
    "![alphago](./images/alphago.png)\n",
    "\n",
    "***Figure5.AlphaGo Zero Architecture***\n",
    "\n",
    "接下来，还要考虑数据结构和数据集的组织，以及设计深度网络结构用来从数据中学习得到Value（对当前情况的评估）和 Policy(接下来Action的策略)。\n",
    "\n",
    "此外，通过还需要考虑探索和利用的平衡问题，比如是要“安全的”反复推荐给用户感兴趣的某一类视频还是要做适当的探索，去了解用户广泛的兴趣点，来使得系统具有更强大的满足用户兴趣需求的推荐能力。\n",
    "\n",
    "最后由于数据稀疏，在线上场景下环境（在此应用中即用户）与推荐系统交互的频率相对较低，所以线上训练较为困难。因此可以通过在线下仿真环境下训练的方式来完成初始训练，然后再上线逐步调优。例如Figure6,所示的采用游戏仿真环境来训练自动驾驶增强学习模型。\n",
    "\n",
    "![Auto-Drive-Pure](./images/autodrive_p.png)\n",
    "\n",
    "***Figure6.Automatic Drive***\n",
    "\n",
    "同样，在抖音信息流推荐的场景中，我们也可以在线下环境中使用其他训练好的信息流推荐模型，比如DSIN模型来模拟用户，给RL系统推荐的内容反馈来完成对RL模型的线下初步训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在强化学习中，Agent需要获得与环境的反馈来不断优化自身对当前状态价值估计和制定最优行动策略。在自动驾驶的应用中，如果让智能体控制实物汽车直接与真实环境进行交互来训练，显然是成本高，危险大，且不太实际的一种做法。所以通常来说，强化学习Agent会在一个对现实世界高仿真，传感器所获数据高仿真的模拟环境下进行开发，训练和验证的。例如,在Intel 和 Toyota 联合开发的自动驾驶模拟环境 CARLA(Car Learning to Act)中， （参考Figure7）。\n",
    "强化学习系统可以从软硬件传感器（软件主要指通过视觉算法增强过的信号收集系统）获得周围环境的深度图像信息，彩色图像信息，场景语义分割信息和雷达型号。\n",
    "\n",
    "![CARLA](./images/autodrive.png)\n",
    "\n",
    "***Figure7. CARLA simulator***\n",
    "\n",
    "基于此，可对强化学习的基本要素进行建模如下：\n",
    "- state: 通过传感器获得的当前时刻或者之前一段时间窗口内的汽车所处环境的信息以及汽车本身的运行状态信息。\n",
    "- action: agent可对汽车控制的操作比如：前进，后退，停车，左转，右转，加速和减速等。\n",
    "- environment：模拟的三维场景和传感器。\n",
    "- reward: 一段时间内正常运行所获得的正向reward，以及因不同事故而获得的负向的reward。\n",
    "\n",
    "设计强化学习的算法框架，从对应用的理解上看，自动驾驶强化学习Agent更适合Model-Based RL。可以采用深度学习对当前的输入信息进行学习来获得价值网络和策略网络，对当前状态进行评估和制定下一步策略。\n",
    "\n",
    "在深度学习方面：\n",
    "1. 可以通过端到端的学习框架，将传感器的输入组织成state直接传递给深度网络，然后根据所获得的reward进行学习。\n",
    "2. 也可以分为两步：\n",
    "    1）将所获得到信息先进行“地图化”，将图形信息转换成一个二维的语义地图；\n",
    "    2）在地图中标识出汽车的位置，临车的未知，障碍物和行人的未知等信息；\n",
    "    3）然后先训练基于抽象信息控制汽车的子网络；\n",
    "    4）最后再将此网络与考虑了直接图像信息网络合并在一起，做价值计算和行为规划。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37664bittfconda946bd5c5de684c5d81ef2ce52df4450d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
