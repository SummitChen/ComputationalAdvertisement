{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *举一个你之前做过的预测例子（用的什么模型，解决什么问题，比如我用LR模型，对员工离职进行了预测，效果如何... 请分享到课程微信群中）*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *XGBoost, LightGBM, CatBoost是三种基于GBDT的实现，三者之间区别是怎样的*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost, LigthGBM 和 CatBoost 是三种对GBDT的改进算法。它们之间的不同点主要体现在所采用的改进措施，以及适用场景和效果的不同。\n",
    "\t\n",
    "XGBoost 算法所采用的改进措施， 在提高优化速度方面有：1. 损失函数用泰勒展开式展开，并用到了一阶和二阶导数； 2. 在寻找最佳分割点时，采用了贪心算法，可以加速计算。3. 支持并行计算。 在增强泛化能力方面, XGBoost 加入了以树模型的复杂度为基础的正则项来避免过拟合。适用场景：处理大规模数据，由于算法参数过多且复杂，不适用处理超高维特征数据。\n",
    "\t\n",
    "LightGBM 通过直方图(Histogram)算法，基于梯度的单边采样算法 (Gradient-based One-side Sampling) 算法 和 互斥特征绑定 (Exclusive Feature Bunding) 算法分别从减少候选分裂点数量，减少样本数量和减少特征数量等发面提高模型的训练效率和减少内存的使用量。\n",
    "\n",
    "CatBoost 适用于有分类特征的场景。设计了一种算法验证改进，用于防止过拟合。\n",
    "\n",
    "从训练速度方面来看，LigthtGBM 最快，CatBoost 次之， XGBoost 最慢。而从预测速度来看，CatBoost > LightGBM > XGBoost。 CatBoost 的过拟合程度最小。在处理分类特征进行预测的能力方面，CatBoost 最好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *你认为，NGBoost对之后的算法会有怎样的影响*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost 采用自然梯度 (natural gradient) 下降的方式来训练模型，除了对结果本身进行预测，同时还对预测结果概率进行了估计。因为减少了结果的不确定性，NGBoost 的预测结果要好于普通梯度(Gradient Boosting) 下降的的方法。NGBoost 所采用的算法思想是概率机器学习，也可以认为NGBoost是概率机器学习的一个典型成功案例。NGBoost 的提出或能促进概率机器学习的发展。所提出的自然梯度方法，可能会广泛应用于改进现有的机器学习模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
